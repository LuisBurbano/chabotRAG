{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc2b9a-6deb-4975-8eb8-7aa05069784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizado por Luis Burbano, Cesar Loor, Sebastian Torres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a82d3-c11c-4ff0-9790-56bfba30c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3afdde-66b3-43b1-b9c8-a707f636376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV con delimitador ;\n",
    "file_path = 'Base_conocimiento_pre.csv'  # Actualiza el |path si es necesario\n",
    "df = pd.read_csv(file_path, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592b8bf-131c-429d-955f-741c5b6c76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3c647-7994-4324-8cf6-6eaf40c828d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar datos faltantes en la columna 'Pregunta'\n",
    "df['Pregunta'] = df['Pregunta'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd9ed2-d83d-470d-ad2a-f9b68f369919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32561570-2cd0-4703-af3f-4e48668884e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo de embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef52c3c-e4c6-49a0-900d-f6eddd601366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear embeddings para las preguntas\n",
    "df['question_embeddings'] = df['Pregunta'].apply(lambda x: model.encode(x).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a320cb-abf7-42ad-a842-5b485b6d4bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8cc6c-7a19-4189-a7fe-55fbd620c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una colección\n",
    "collection = client.create_collection(name=\"qa_collection2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833225ec-1df5-492a-a69f-86f398e562f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar vectores a la colección\n",
    "for index, row in df.iterrows():\n",
    "    collection.add(\n",
    "        documents=[row['Pregunta']],\n",
    "        embeddings=[row['question_embeddings']],\n",
    "        ids=[str(index)]  # Cambiado metadatas a ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a1af2-b7c8-47cb-b24a-7c1139b8cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "    \"\"\"\n",
    "    Obtiene la mejor respuesta para una pregunta dada usando la colección y el modelo de embeddings.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode(pregunta).tolist()\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=1)\n",
    "    \n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "    \n",
    "    if not ids_list:\n",
    "        return None, float('inf')\n",
    "    \n",
    "    best_result = sorted(zip(ids_list, distances_list), key=lambda x: x[1])[0]\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "    \n",
    "    return respuesta, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295919ec-2f8a-43b8-a07d-d8d791ac2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo Llama3\n",
    "llm = Ollama(model=\"llama3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228051e-1ee8-4c3e-bf4c-30d97c562641",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", basándote en la información proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c95726-f542-4d53-9eea-4f77c7e4a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e03c4-1099-4827-9ebe-bd974e04e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5  # Ajusta este umbral según sea necesario\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            return\n",
    "        \n",
    "        # Primero, intenta obtener una respuesta de Chroma\n",
    "        answer, distance = obtener_respuesta(pregunta, collection, df, model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            response = answer\n",
    "        else:\n",
    "            # Si no hay respuesta en Chroma o la similitud es baja, utiliza el modelo Llama3\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "        \n",
    "        chat_history.append(HumanMessage(content=pregunta))\n",
    "        chat_history.append(AIMessage(content=response))\n",
    "        print(\"_\" * 50)\n",
    "        print(\"IA: \" + response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae68f3-4495-47c0-9b4c-600763ab8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "# Convertir el DataFrame en documentos\n",
    "documents = []\n",
    "for index, row in df.iterrows():\n",
    "    # Suponiendo que la columna 'text' contiene el contenido relevante\n",
    "    content = row['Respuesta']\n",
    "    document = Document(page_content=content)\n",
    "    documents.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ab624-9740-4d45-b8db-2242c93e95bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Configura el modelo de embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crea los embeddings para los documentos\n",
    "document_embeddings = embedding_model.embed_documents([doc.page_content for doc in documents])\n",
    "\n",
    "# Configura la base de datos vectorial\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding_model,\n",
    "    collection_name=\"my_collection\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9814eba-ef20-4253-ba13-6ebfa61bedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Configura el modelo Llama3\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Configura el template del prompt\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", basándote en la información proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342a483-9842-4005-8561-b45c79ed0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5  # Ajusta este umbral según sea necesario\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            return\n",
    "        \n",
    "        # Primero, intenta obtener una respuesta de Chroma\n",
    "        answer, distance = obtener_respuesta(pregunta, collection, df, embedding_model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            response = answer\n",
    "        else:\n",
    "            # Si no hay respuesta en Chroma o la similitud es baja, utiliza el modelo Llama3\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "        \n",
    "        # Verifica si response es un diccionario y extrae el texto adecuado\n",
    "        if isinstance(response, dict):\n",
    "            response_text = response.get('text', 'No response text found')\n",
    "        else:\n",
    "            response_text = response\n",
    "        \n",
    "        chat_history.append({\"role\": \"user\", \"content\": pregunta})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "        print(\"_\" * 50)\n",
    "        print(\"IA: \" + response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645c646-46cb-4a9b-a9ba-d605dd730536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7419d1-8183-4d2c-a1d2-23a776ce94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "st.title(\"Chat with Webpage 🌐\")\n",
    "st.caption(\"This app allows you to chat with a webpage using local Llama-3 and RAG\")\n",
    "\n",
    "webpage_url = st.text_input(\"Enter Webpage URL\", type=\"default\")\n",
    "\n",
    "if webpage_url:\n",
    "    loader = WebBaseLoader(webpage_url)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    " \n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    def ollama_llm(question, context):\n",
    "        formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "        response = ollama.chat(model='llama3', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "        return response['message']['content']\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    def combine_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    def rag_chain(question):\n",
    "        retrieved_docs = retriever.invoke(question)\n",
    "        formatted_context = combine_docs(retrieved_docs)\n",
    "        return ollama_llm(question, formatted_context)\n",
    "\n",
    "    st.success(f\"Loaded {webpage_url} successfully!\")\n",
    "    prompt = st.text_input(\"Ask any question about the webpage\")\n",
    "\n",
    "    if prompt:\n",
    "        result = rag_chain(prompt)\n",
    "        st.write(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0c0c7-6e35-4de5-9ea3-7ae4ad36bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Configuración de las variables de entorno\n",
    "os.environ['CHROMA_DB_PATH'] = './chroma_db_data'\n",
    "\n",
    "# Cargar datos desde un CSV usando pandas\n",
    "file_path = 'Base_conocimiento_pre.csv'\n",
    "df2 = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Suponiendo que la columna de interés se llama 'Respuesta'\n",
    "texts = df2['Respuesta'].tolist()\n",
    "\n",
    "# Convertir textos a objetos Document\n",
    "documents = [Document(page_content=text) for text in texts]\n",
    "\n",
    "# Dividir documentos en fragmentos\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Crear el cliente de Chroma DB y el vector store\n",
    "chroma_collection = Chroma.from_documents(documents=splits, embedding=OllamaEmbeddings(model=\"llama3\"))\n",
    "\n",
    "# Inicializar Ollama y el contexto de servicio\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Crear el prompt para el QA\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Question: {question}\\n\\nContext: {context}\"\n",
    ")\n",
    "\n",
    "# Crear la cadena RetrievalQA\n",
    "retriever = chroma_collection.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# Función para realizar consultas\n",
    "def perform_query(query):\n",
    "    response = qa_chain({\"question\": query})\n",
    "    return response['result']\n",
    "\n",
    "# Realizar una consulta y mostrar la respuesta\n",
    "query = \"What are the thoughts on food quality?\"\n",
    "response = perform_query(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979c95f-12f4-47b5-aee8-6c7cf7fcc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2f7f1-5513-4882-bcd3-e3d473e66323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración de las variables de entorno\n",
    "os.environ['CHROMA_DB_PATH'] = './chroma_db_data'\n",
    "\n",
    "# Cargar datos desde un CSV usando pandas\n",
    "file_path = 'Base_conocimiento_pre.csv'\n",
    "df = pd.read_csv(file_path, delimiter=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5bc6e-a965-405a-9c19-aebb85df0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Cargar el modelo para crear embeddings\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# Crear embeddings para preguntas y respuestas\n",
    "df['embedding'] = df['Pregunta'].apply(lambda x: model.encode(x).tolist())\n",
    "\n",
    "# Inicializar Chroma sin `collection`\n",
    "client = chromadb.Client(Settings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491620ff-1fc5-4c56-910c-326d2f4cbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir documentos en fragmentos\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Crear el cliente de Chroma DB y el vector store\n",
    "chroma_collection = Chroma.from_documents(documents=splits, embedding=OllamaEmbeddings(model=\"llama3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e140df4-cfd9-4508-968b-19f3d2fd3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Ollama y el contexto de servicio\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a03c6-66d6-4c7d-9c6b-237c8da761c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Configuración del modelo de embeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Invoke chain with RAG context\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Load page content\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Vector store things\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "vector_store = Chroma.from_documents(split_documents, hf)\n",
    "\n",
    "# Prompt construction\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question only based on the given context\n",
    "    \n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Retrieve context from vector store\n",
    "docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, docs_chain)\n",
    "\n",
    "# Winner winner chicken dinner\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(\":::ROUND 2:::\")\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f2063-931b-46b8-95f0-c85cfc4c85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Configura el modelo de embeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Configura el CSVLoader para tu archivo\n",
    "loader = CSVLoader(\n",
    "    file_path='./Base_conocimiento_pre.csv',\n",
    "    csv_args={\n",
    "        'delimiter': ';',\n",
    "        'quotechar': '\"',\n",
    "        'fieldnames': ['Index', 'Pregunta', 'Respuesta']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "docs = loader.load()\n",
    "\n",
    "# Vector store things\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "vector_store = Chroma.from_documents(split_documents, hf)\n",
    "\n",
    "# Prompt construction\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question only based on the given context\n",
    "    \n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Retrieve context from vector store\n",
    "llm = Ollama(model=\"llama3\")\n",
    "docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, docs_chain)\n",
    "\n",
    "# Winner winner chicken dinner\n",
    "response = retrieval_chain.invoke({\"input\": \"¿Dónde se encuentra el departamento de computación?\"})\n",
    "print(\":::ROUND 2:::\")\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b0a577-e474-4379-bc99-ec143f1c861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = './Base_conocimiento_pre.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path, delimiter=';')\n",
    "    print(\"Archivo CSV cargado con Pandas.\")\n",
    "    print(df.head())  # Muestra las primeras filas del DataFrame para verificar\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el archivo CSV con Pandas: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b542eeb-d9eb-4d55-897a-4b60759a5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo CSV\n",
    "file_path = './Base_conocimiento_pre.csv'\n",
    "\n",
    "# Cargar el archivo CSV con pandas\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11bce00-e4a3-4cc2-bcb7-8019ea69716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Ruta del archivo CSV\n",
    "file_path = './Base_conocimiento_pre.csv'\n",
    "\n",
    "# Cargar el archivo CSV con pandas\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Convertir DataFrame a una lista de documentos\n",
    "docs = [Document(page_content=row['Respuesta'], metadata={\"id\": row['n']}) for index, row in df.iterrows()]\n",
    "\n",
    "print(f\"Documentos cargados: {len(docs)}\")\n",
    "\n",
    "# Configurar el modelo de embeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Configurar el vector store con Chroma\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "vector_store = Chroma.from_documents(split_documents, hf_embeddings)\n",
    "\n",
    "# Configurar el prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question only based on the given context\n",
    "    \n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Configurar el modelo LLM\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Configurar la cadena de recuperación\n",
    "docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, docs_chain)\n",
    "\n",
    "# Realizar una consulta\n",
    "response = retrieval_chain.invoke({\"input\": \"¿Dónde se encuentra el departamento de computación?\"})\n",
    "print(\":::ROUND 2:::\")\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba6ef7-bc55-464f-a60f-47c5631c5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una consulta\n",
    "response = retrieval_chain.invoke({\"input\": \"¿Cuáles son mis derechos como estudiante?\"})\n",
    "print(\":::ROUND 2:::\")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa412362-1787-49ae-903c-ead7f7b20e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una consulta\n",
    "response = retrieval_chain.invoke({\"input\": \"¿Entonces que pasa si tengo un problema con un profesor?\"})\n",
    "print(\":::ROUND 2:::\")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6dee9-10e5-4948-8731-63c20e2b43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una consulta\n",
    "response = retrieval_chain.invoke({\"input\": \"Quiero hacer pasantias en la universidad pero no se con quien y soy de software¿Con quien debo hablar?\"})\n",
    "print(\":::ROUND 2:::\")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17356abf-e1b7-4088-9221-412005f0d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una consulta\n",
    "response = retrieval_chain.invoke({\"input\": \"¿Dónde se encuentra el psicólogo de la universidad?\"})\n",
    "print(\":::ROUND 2:::\")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ef070-9bc7-4bf6-a543-d5426ad08d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Cargar el archivo CSV con delimitador ;\n",
    "file_path = 'Base_conocimiento_pre.csv'\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "df['Pregunta'] = df['Pregunta'].fillna('')\n",
    "\n",
    "# Dividir los datos en fragmentos si es necesario\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = [Document(page_content=row['Pregunta']) for _, row in df.iterrows()]\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Crear embeddings para las preguntas\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "\n",
    "try:\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "except Exception as ex:\n",
    "    print(\"Exception: \", ex)\n",
    "    local_model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    print(f\"Use alternative (local) model: {local_model_path}\\n\")\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)\n",
    "\n",
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)\n",
    "collection = client.create_collection(name=\"qa_collection\")\n",
    "\n",
    "# Generar embeddings para todos los fragmentos\n",
    "all_embeddings = [embeddings_model.embed_documents([doc.page_content])[0] for doc in all_splits]\n",
    "ids = [str(i) for i in range(len(all_splits))]\n",
    "\n",
    "# Agregar todos los documentos a la colección\n",
    "collection.add(\n",
    "    documents=[doc.page_content for doc in all_splits],\n",
    "    embeddings=all_embeddings,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "    # Obtener el embedding de la pregunta\n",
    "    query_embedding = model.embed_documents([pregunta])[0]\n",
    "\n",
    "    # Realizar la consulta en la colección usando similarity_search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],  # Consulta con embeddings\n",
    "        n_results=3  # Número de resultados a devolver\n",
    "    )\n",
    "\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:\n",
    "        return None, float('inf'), []\n",
    "\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "    best_result = sorted_results[0]\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "\n",
    "    # Contexto adicional\n",
    "    additional_context = [df.loc[int(doc_id), 'Respuesta'] for doc_id, _ in sorted_results[1:3]]\n",
    "\n",
    "    return respuesta, distance, additional_context\n",
    "\n",
    "# Configuración del modelo Ollama\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", basándote en la información proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat. Además siempre debes responder en español\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            break\n",
    "        \n",
    "        answer, distance, additional_context = obtener_respuesta(pregunta, collection, df, embeddings_model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}. Información adicional: {' '.join(additional_context)}\"\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=answer))\n",
    "            print(f\"Espesito: {answer}\")\n",
    "        else:\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=response))\n",
    "            print(f\"Espesito: {response}\")\n",
    "\n",
    "# Ejecutar el chat\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1133ae-effc-4ae7-b59a-9306a491ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a1b66-b359-4928-809f-b25212ff7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Cargar el archivo CSV con delimitador ;\n",
    "file_path = 'Base_conocimiento_pre.csv'\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "df['Pregunta'] = df['Pregunta'].fillna('')\n",
    "\n",
    "# Dividir los datos en fragmentos si es necesario\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = [Document(page_content=row['Pregunta']) for _, row in df.iterrows()]\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Crear embeddings para las preguntas\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "\n",
    "try:\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "except Exception as ex:\n",
    "    print(\"Exception: \", ex)\n",
    "    local_model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    print(f\"Use alternative (local) model: {local_model_path}\\n\")\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)\n",
    "\n",
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)\n",
    "collection = client.create_collection(name=\"qa_collection\")\n",
    "\n",
    "# Generar embeddings para todos los fragmentos\n",
    "all_embeddings = [embeddings_model.embed_documents([doc.page_content])[0] for doc in all_splits]\n",
    "ids = [str(i) for i in range(len(all_splits))]\n",
    "\n",
    "# Agregar todos los documentos a la colección\n",
    "collection.add(\n",
    "    documents=[doc.page_content for doc in all_splits],\n",
    "    embeddings=all_embeddings,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "    # Obtener el embedding de la pregunta\n",
    "    query_embedding = model.embed_documents([pregunta])[0]\n",
    "\n",
    "    # Realizar la consulta en la colección usando similarity_search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],  # Consulta con embeddings\n",
    "        n_results=3  # Número de resultados a devolver\n",
    "    )\n",
    "\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:\n",
    "        return None, float('inf'), []\n",
    "\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "    best_result = sorted_results[0]\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "\n",
    "    # Contexto adicional\n",
    "    additional_context = [df.loc[int(doc_id), 'Respuesta'] for doc_id, _ in sorted_results[1:3]]\n",
    "\n",
    "    return respuesta, distance, additional_context\n",
    "\n",
    "# Configuración de los modelos Ollama\n",
    "llm_gemma2 = Ollama(model=\"gemma2\")\n",
    "llm_llama3 = Ollama(model=\"llama3\")\n",
    "\n",
    "prompt_template_gemma2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", basándote en la información proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_template_llama3 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas detalladas y complejas sobre la Universidad de las Fuerzas Armadas \"ESPE\", basándote en la información proporcionada en las preguntas frecuentes.\n",
    "            Usa toda la información que tengas disponible para proporcionar una respuesta completa y precisa.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain_gemma2 = prompt_template_gemma2 | llm_gemma2\n",
    "chain_llama3 = prompt_template_llama3 | llm_llama3\n",
    "\n",
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            break\n",
    "        \n",
    "        # Determinar la complejidad de la pregunta\n",
    "        if len(pregunta.split()) > 10:  # Umbral de complejidad\n",
    "            model = llm_llama3\n",
    "            chain = chain_llama3\n",
    "        else:\n",
    "            model = llm_gemma2\n",
    "            chain = chain_gemma2\n",
    "\n",
    "        answer, distance, additional_context = obtener_respuesta(pregunta, collection, df, embeddings_model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}. Información adicional: {' '.join(additional_context)}\"\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=answer))\n",
    "            print(f\"Espesito: {answer}\")\n",
    "        else:\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=response))\n",
    "            print(f\"Espesito: {response}\")\n",
    "\n",
    "# Ejecutar el chat\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53dea81d-ad7f-48b5-ac42-36ea8ddcfd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from pdfminer.high_level import extract_text\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daaa3c78-90fc-4b08-8de6-3e1c2cd4797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "def extraer_texto_pdf(pdf_path):\n",
    "    texto_completo = \"\"\n",
    "    with open(pdf_path, 'rb') as archivo_pdf:\n",
    "        lector_pdf = PyPDF2.PdfReader(archivo_pdf)\n",
    "        for pagina in lector_pdf.pages:\n",
    "            texto_completo += pagina.extract_text()\n",
    "    return texto_completo\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    # Eliminar saltos de línea innecesarios y caracteres especiales\n",
    "    texto = re.sub(r'\\n+', '\\n', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    texto = re.sub(r'[^\\x00-\\x7F]+', ' ', texto)  # Eliminar caracteres no ASCII\n",
    "    return texto.strip()\n",
    "\n",
    "def segmentar_texto(texto):\n",
    "    # Puedes segmentar el texto en fragmentos lógicos basados en títulos o párrafos\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    \n",
    "    # Opcional: Agregar lógica para identificar y mantener secciones importantes\n",
    "    secciones = re.split(r'\\n\\s*\\n', texto)  # Dividir por párrafos\n",
    "    \n",
    "    documentos = []\n",
    "    for seccion in secciones:\n",
    "        # Puedes agregar etiquetas para secciones importantes si es necesario\n",
    "        fragmentos = text_splitter.split_text(seccion)\n",
    "        documentos.extend([Document(page_content=frag) for frag in fragmentos])\n",
    "    \n",
    "    return documentos\n",
    "\n",
    "# Función para extraer y procesar texto de una página web\n",
    "def cargar_web(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extraer el contenido de la página web\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Dividir el texto en fragmentos si es necesario\n",
    "    documents = [Document(page_content=text)]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b0dab5-69d6-409a-b658-c83b8beea6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Universidad de las Fuerzas Armadas ( ESPE ), tiene su origen cuando el 16 de junio de 1922 se crea la  Escuela de Oficiales de Ingenieros , mediante Decreto del Presidente de la Rep blica, Dr. Jos  Luis Tamayo, publicado en el Registro Oficial 521; en vista de la necesidad de tecnificar los mandos en las especialidades de ingenier a y artiller a el 22 de octubre de 1936, durante la Presidencia de Federico P ez, se cambia el nombre por  Escuela de Artiller a e Ingenieros . Al ampliar su pensum acad mico y nivelarlo con las dem s universidades ecuatorianas, en 1948 se la denomin  Escuela T cnica de Ingenieros. Ante la crisis universitaria del pa s y las necesidades de las Fuerzas Armadas, en 1972 el Gral. Guillermo Rodr guez Lara, Presidente de la Rep blica abre las puertas para que ingresen es tudiantes civiles. El Depar tamento de Ciencias de la C omputaci n (DCCO ) es el departamento encargado de las carreras de Ingenier a en Software y T ecnolog  as d e la i nformaci  n. Algunas\n",
      "encargado de las carreras de Ingenier a en Software y T ecnolog  as d e la i nformaci  n. Algunas de las preguntas frecuentes y su respectiva respuesta que suelen hacer en la Universidad son:  Qu  carreras ofrece la Universidad de las Fuerzas Armadas ESPE? La ESPE ofrece una variedad de carreras en ingenier a , ciencias de la vida, ciencias administrativas, ciencias de la tierra, y m  s.  Cu l es el proceso de admisi  n para la ESPE? El proceso de admisi  n incluye la inscripci n en el sistema nacional de nivelaci n y admisi n, la rendici n del examen de ingreso, y la selecci n basada en el puntaje obtenido.\n"
     ]
    }
   ],
   "source": [
    "# Ruta al archivo PDF\n",
    "pdf_path = 'preguntas.pdf'\n",
    "\n",
    "# Proceso de preprocesamiento\n",
    "texto_extraido = extraer_texto_pdf(pdf_path)\n",
    "texto_limpio = limpiar_texto(texto_extraido)\n",
    "documentos_segmentados = segmentar_texto(texto_limpio)\n",
    "\n",
    "# Ahora tienes una lista de `Document` listos para cargar en Chroma\n",
    "for documento in documentos_segmentados:\n",
    "    print(documento.page_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50d094f-2197-420b-9faa-abf477641d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y combinar el contenido del PDF y la web\n",
    "web_documents = cargar_web('https://www.espe.edu.ec/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c34b9ee-2482-4625-aeec-7218c529c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar todos los documentos\n",
    "all_documents = documentos_segmentados + web_documents\n",
    "\n",
    "# Dividir los documentos en fragmentos si es necesario\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(all_documents)\n",
    "\n",
    "# Crear embeddings para los fragmentos\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f78f752-cbc6-49dc-874c-5d38a81171b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "except Exception as ex:\n",
    "    print(\"Exception: \", ex)\n",
    "    local_model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d449b95e-f256-42bf-a56e-092b73348f4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m collection \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_collection(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Crear embeddings y cargar en Chroma\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [embeddings_model\u001b[38;5;241m.\u001b[39membed_documents([doc\u001b[38;5;241m.\u001b[39mpage_content])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mall\u001b[39m]\n\u001b[0;32m      8\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_documents))]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Agregar todos los documentos a la colección\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)\n",
    "collection = client.create_collection(name=\"qa_collection\")\n",
    "\n",
    "# Crear embeddings y cargar en Chroma\n",
    "all_embeddings = [embeddings_model.embed_documents([doc.page_content])[0] for doc in all]\n",
    "ids = [str(i) for i in range(len(all_documents))]\n",
    "\n",
    "# Agregar todos los documentos a la colección\n",
    "collection.add(\n",
    "    documents=[doc.page_content for doc in all_documents],\n",
    "    embeddings=all_embeddings,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81fde9-1d10-4804-8275-336d5ea73e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de los modelos Ollama\n",
    "llm_gemma2 = Ollama(model=\"gemma2\")\n",
    "llm_llama3 = Ollama(model=\"llama3\")\n",
    "\n",
    "prompt_template_gemma2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", basándote en la información proporcionada en las preguntas frecuentes.\n",
    "        Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_template_llama3 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas detalladas y complejas sobre la Universidad de las Fuerzas Armadas \"ESPE\", basándote en la información proporcionada en las preguntas frecuentes.\n",
    "        Usa toda la información que tengas disponible para proporcionar una respuesta completa y precisa.\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3af25b-6f77-4443-9f79-3fc10b9a25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_gemma2 = prompt_template_gemma2 | llm_gemma2\n",
    "chain_llama3 = prompt_template_llama3 | llm_llama3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565cb8e-d115-4205-9d7d-759c9cea4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_respuesta(pregunta: str, collection, embeddings_model, umbral_similitud=0.3):\n",
    "    # Crear embedding para la pregunta\n",
    "    query_embedding = embeddings_model.embed_documents([pregunta])[0]\n",
    "    \n",
    "    # Realizar la consulta en la colección\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=3  # Aumentar el número de resultados para tener más contexto\n",
    "    )\n",
    "    \n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:\n",
    "        return None, float('inf'), []\n",
    "\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "    best_result = sorted_results[0]\n",
    "    doc_id, distance = best_result\n",
    "    \n",
    "    # Obtener la respuesta más cercana y verificar si está por debajo del umbral\n",
    "    if distance < umbral_similitud:\n",
    "        index = int(doc_id)\n",
    "        respuesta = collection.get_documents([doc_id])[0]\n",
    "        additional_context = [collection.get_documents([doc_id])[0] for doc_id, _ in sorted_results[1:3]]\n",
    "        return respuesta, distance, additional_context\n",
    "    else:\n",
    "        return None, float('inf'), []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305070e-3161-425b-8276-9f8d8590a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5  # Ajusta este umbral según sea necesario\n",
    "    \n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        \n",
    "        if pregunta.lower() == \"adios\":\n",
    "            print(\"IA: ¡Adiós! Ha sido un placer hablar contigo.\")\n",
    "            break\n",
    "        \n",
    "        # Primero, intenta obtener una respuesta de Chroma\n",
    "        answer, distance, additional_context = obtener_respuesta(pregunta, collection, embeddings_model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}. Información adicional: {' '.join(additional_context)}\"\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=answer))\n",
    "            response = chain_llama3.invoke({\"input\": context, \"chat_history\": chat_history})\n",
    "        else:\n",
    "            # Si no hay respuesta en Chroma o la similitud es baja, utiliza el modelo Llama3 o Gemma2\n",
    "            if len(pregunta.split()) > 10:\n",
    "                response = chain_llama3.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "            else:\n",
    "                response = chain_gemma2.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "\n",
    "        chat_history.append(HumanMessage(content=pregunta))\n",
    "        chat_history.append(AIMessage(content=response))\n",
    "        \n",
    "        print(\"_\" * 50)\n",
    "        print(\"IA: \" + response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba31ec-8628-4c51-b5aa-5e7791eb0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5418d04-2549-4c8d-a8b5-8a3b07212d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import PyPDF2\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "import torch\n",
    "\n",
    "# Función para extraer texto del PDF\n",
    "def extraer_texto_pdf(pdf_path):\n",
    "    texto_completo = \"\"\n",
    "    with open(pdf_path, 'rb') as archivo_pdf:\n",
    "        lector_pdf = PyPDF2.PdfReader(archivo_pdf)\n",
    "        for pagina in lector_pdf.pages:\n",
    "            texto_completo += pagina.extract_text()\n",
    "    return texto_completo\n",
    "\n",
    "# Función para extraer texto de la web\n",
    "def extraer_texto_web(url):\n",
    "    respuesta = requests.get(url)\n",
    "    sopa = BeautifulSoup(respuesta.content, 'html.parser')\n",
    "    texto = sopa.get_text()\n",
    "    return texto\n",
    "\n",
    "# Función para limpiar el texto\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r'\\n+', '\\n', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    texto = re.sub(r'[^\\x00-\\x7F]+', ' ', texto)  # Eliminar caracteres no ASCII\n",
    "    return texto.strip()\n",
    "\n",
    "# Función para segmentar el texto en fragmentos\n",
    "def segmentar_texto(texto):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    secciones = re.split(r'\\n\\s*\\n', texto)  # Dividir por párrafos\n",
    "    documentos = []\n",
    "    for seccion in secciones:\n",
    "        fragmentos = text_splitter.split_text(seccion)\n",
    "        documentos.extend([Document(page_content=frag) for frag in fragmentos])\n",
    "    return documentos\n",
    "\n",
    "# Configuración de Chroma y modelos\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)\n",
    "collection = client.create_collection(name=\"qa_collection_combined\")\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "\n",
    "try:\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "except Exception as ex:\n",
    "    print(\"Exception: \", ex)\n",
    "    local_model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)\n",
    "\n",
    "llm_gemma2 = Ollama(model=\"gemma2\")\n",
    "llm_llama3 = Ollama(model=\"llama3\")\n",
    "\n",
    "prompt_template_gemma2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", basándote en la información proporcionada en las preguntas frecuentes.\n",
    "        Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_template_llama3 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas detalladas y complejas sobre la Universidad de las Fuerzas Armadas \"ESPE\", basándote en la información proporcionada en las preguntas frecuentes.\n",
    "        Usa toda la información que tengas disponible para proporcionar una respuesta completa y precisa.\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain_gemma2 = prompt_template_gemma2 | llm_gemma2\n",
    "chain_llama3 = prompt_template_llama3 | llm_llama3\n",
    "\n",
    "# Procesar PDF\n",
    "pdf_path = 'preguntas.pdf'\n",
    "texto_extraido_pdf = extraer_texto_pdf(pdf_path)\n",
    "texto_limpio_pdf = limpiar_texto(texto_extraido_pdf)\n",
    "documentos_segmentados_pdf = segmentar_texto(texto_limpio_pdf)\n",
    "\n",
    "# Procesar web\n",
    "url_web = 'https://www.espe.edu.ec/'\n",
    "texto_extraido_web = extraer_texto_web(url_web)\n",
    "texto_limpio_web = limpiar_texto(texto_extraido_web)\n",
    "documentos_segmentados_web = segmentar_texto(texto_limpio_web)\n",
    "\n",
    "# Combinar documentos de PDF y web\n",
    "documentos_combinados = documentos_segmentados_pdf + documentos_segmentados_web\n",
    "\n",
    "# Crear embeddings y cargar en Chroma\n",
    "all_embeddings = [embeddings_model.embed_documents([doc.page_content])[0] for doc in documentos_combinados]\n",
    "ids = [str(i) for i in range(len(documentos_combinados))]\n",
    "\n",
    "collection.add(\n",
    "    documents=[doc.page_content for doc in documentos_combinados],\n",
    "    embeddings=all_embeddings,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "def obtener_respuesta(pregunta: str, collection, model):\n",
    "    query_embedding = model.embed_documents([pregunta])[0]\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=3\n",
    "    )\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:\n",
    "        return None, float('inf')\n",
    "\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "    best_result = sorted_results[0]\n",
    "    doc_id, distance = best_result\n",
    "    respuesta = documentos_combinados[int(doc_id)].page_content\n",
    "    return respuesta, distance\n",
    "\n",
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5  # Ajusta este umbral según sea necesario\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            return\n",
    "        \n",
    "        # Primero, intenta obtener una respuesta de Chroma\n",
    "        answer, distance = obtener_respuesta(pregunta, collection, embeddings_model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            response = answer\n",
    "        else:\n",
    "            # Si no hay respuesta en Chroma o la similitud es baja, utiliza el modelo Llama3\n",
    "            response = chain_llama3.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "        \n",
    "        chat_history.append(HumanMessage(content=pregunta))\n",
    "        chat_history.append(AIMessage(content=response))\n",
    "        print(\"_\" * 50)\n",
    "        print(\"IA: \" + response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8502ce26-fff0-45dc-bf46-c73af2565118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "def extraer_texto_pdf(pdf_path):\n",
    "    # Abrir y leer el archivo PDF\n",
    "    texto = \"\"\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page in reader.pages:\n",
    "            texto += page.extract_text()\n",
    "    return texto\n",
    "\n",
    "def extraer_texto_web(url):\n",
    "    # Obtener el contenido de la página web\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Extraer todo el texto de la página\n",
    "    texto = soup.get_text()\n",
    "    return texto\n",
    "\n",
    "# Cargar el contenido del PDF y la Web\n",
    "texto_pdf = extraer_texto_pdf('preguntas.pdf')\n",
    "texto_web = extraer_texto_web('https://www.espe.edu.ec/')\n",
    "\n",
    "# Combina el texto de ambas fuentes\n",
    "texto_combinado = texto_pdf + \" \" + texto_web\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d23f5cd-693f-421d-96cb-5a2ac26de825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Configuración de Chroma y el modelo de embeddings\n",
    "client = Client(Settings())\n",
    "collection = client.create_collection(name=\"mi_coleccion\")\n",
    "\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Divide el texto en fragmentos si es necesario\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = text_splitter.split_text(texto_combinado)\n",
    "\n",
    "# Generar embeddings para todos los fragmentos\n",
    "all_splits = [Document(page_content=doc) for doc in documents]\n",
    "all_embeddings = [embeddings_model.embed_documents([doc.page_content])[0] for doc in all_splits]\n",
    "ids = [str(i) for i in range(len(all_splits))]\n",
    "\n",
    "# Agregar todos los documentos a la colección en Chroma\n",
    "collection.add(\n",
    "    documents=[doc.page_content for doc in all_splits],\n",
    "    embeddings=all_embeddings,\n",
    "    ids=ids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "295d8d28-d241-403e-84df-dc05a95c29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_respuesta(pregunta, collection, embeddings_model):\n",
    "    pregunta_embedding = embeddings_model.embed_query(pregunta)\n",
    "    results = collection.query(query_embeddings=[pregunta_embedding], n_results=3)\n",
    "\n",
    "    if results['documents']:\n",
    "        # Selecciona la mejor respuesta\n",
    "        respuesta = results['documents'][0][0]  # Accede al primer documento del primer resultado\n",
    "        distancia = results['distances'][0][0]  # Accede a la primera distancia del primer resultado\n",
    "        contexto_adicional = [doc[0] for doc in results['documents'][1:]]  # Extrae el contexto adicional si existe\n",
    "        return respuesta, distancia, contexto_adicional\n",
    "    else:\n",
    "        return None, float('inf'), []  # Devolver una distancia infinita si no se encuentra nada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151d6ab-39f3-4a10-b3e5-c719e4d7a07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0a63b5-64a1-488e-865a-1d6eb3f76790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, data_dir_path=\"data\", db_path=\"chroma\") -> None:\n",
    "        print(\"Initializing RAG System\")\n",
    "        self.data_directory = data_dir_path\n",
    "        self.db_path = db_path\n",
    "        self.model_name = \"llama3\"\n",
    "        self.llm_model = \"llama3\"\n",
    "        self.model = Ollama(model=self.llm_model)\n",
    "        self.prompt_template = \"\"\"\n",
    "                                Eres una IA llamada Espesito que se centra en responder preguntas pregunta sobre la\n",
    "                                Universidad de las Fuerzas Armadas ESPE, debes dar respuestas simples a las preguntas y siempre que puedas usa \n",
    "                                la información que obtengas del historial del chat, además siempre debes responder en español\n",
    "\n",
    "                                {context}\n",
    "\n",
    "                                Question: {question}\n",
    "                                Answer:\n",
    "                                \"\"\"\n",
    "        self._setup_collection()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        pages = self._load_documents()\n",
    "        chunks = self._document_splitter(pages)\n",
    "        chunks = self._get_chunk_ids(chunks)\n",
    "        vectordb = self._initialize_vectorDB()\n",
    "        present_in_db = vectordb.get()\n",
    "        ids_in_db = present_in_db[\"ids\"]\n",
    "        print(f\"Number of existing ids in db: {len(ids_in_db)}\")\n",
    "        # Add chunks to db - check if they already exist\n",
    "        chunks_to_add = [i for i in chunks if i.metadata.get(\"chunk_id\") not in ids_in_db]\n",
    "        if len(chunks_to_add) > 0:\n",
    "            vectordb.add_documents(chunks_to_add, ids=[i.metadata[\"chunk_id\"] for i in chunks_to_add])\n",
    "            print(f\"Added to db: {len(chunks_to_add)} records\")\n",
    "            vectordb.persist()\n",
    "        else:\n",
    "            print(\"No records to add\")\n",
    "\n",
    "    def _get_chunk_ids(self, chunks):\n",
    "        prev_page_id = None\n",
    "        for i in chunks:\n",
    "            src = i.metadata.get(\"source\")\n",
    "            page = i.metadata.get(\"page\")\n",
    "            curr_page_id = f\"{src}_{page}\"\n",
    "            if curr_page_id == prev_page_id:\n",
    "                curr_chunk_index += 1\n",
    "            else:\n",
    "                curr_chunk_index = 0\n",
    "            curr_chunk_id = f\"{curr_page_id}_{curr_chunk_index}\"\n",
    "            prev_page_id = curr_page_id\n",
    "            i.metadata[\"chunk_id\"] = curr_chunk_id\n",
    "        return chunks        \n",
    "\n",
    "    def _retrieve_context_from_query(self, query_text):\n",
    "        vectordb = self._initialize_vectorDB()\n",
    "        context = vectordb.similarity_search_with_score(query_text, k=1)\n",
    "        return context\n",
    "    \n",
    "    def _get_prompt(self, query_text):\n",
    "        context = self._retrieve_context_from_query(query_text)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in context])\n",
    "        prompt_template = ChatPromptTemplate.from_template(self.prompt_template)\n",
    "        prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "        return prompt\n",
    "\n",
    "    def answer_query(self, query_text):\n",
    "        prompt = self._get_prompt(query_text)\n",
    "        response_text = self.model.invoke(prompt)\n",
    "        formatted_response = f\"Response: {response_text}\\n\"\n",
    "        return formatted_response\n",
    "\n",
    "    def _load_documents(self):\n",
    "        loader = PyPDFDirectoryLoader(self.data_directory)\n",
    "        pages = loader.load()\n",
    "        return pages\n",
    "\n",
    "    def _document_splitter(self, documents):\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=400,\n",
    "            chunk_overlap=100,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        return splitter.split_documents(documents)\n",
    "    \n",
    "    def _get_embedding_func(self):\n",
    "        embeddings = OllamaEmbeddings(model=self.model_name)\n",
    "        return embeddings\n",
    "    \n",
    "    def _initialize_vectorDB(self):\n",
    "        return Chroma(\n",
    "            persist_directory=self.db_path,\n",
    "            embedding_function=self._get_embedding_func(),\n",
    "        )\n",
    "\n",
    "def chat():\n",
    "    rag_system = RAGSystem()\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5  # Ajusta este umbral según sea necesario\n",
    "\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            return\n",
    "        \n",
    "        # Seleccionar el modelo y la cadena en función de la longitud de la pregunta\n",
    "        if len(pregunta.split()) > 10:\n",
    "            model = rag_system.llm_model\n",
    "            modelo_usado = \"Llama3\"\n",
    "        else:\n",
    "            model = rag_system.llm_model\n",
    "            modelo_usado = \"Gemma2\"\n",
    "\n",
    "        # Mostrar el modelo que va a responder\n",
    "        print(f\"Modelo seleccionado: {modelo_usado}\")\n",
    "\n",
    "        # Intentar obtener una respuesta de Chroma basada en los embeddings del PDF y la web\n",
    "        answer, distance, additional_context = rag_system._retrieve_context_from_query(pregunta)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            # Aquí refinamos la respuesta con el modelo LLM, pero usando solo el contexto del PDF o web\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}. Información adicional: {' '.join(additional_context)}\"\n",
    "            chat_history.append({\"role\": \"user\", \"content\": pregunta})\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "            \n",
    "            refined_response = rag_system.model.invoke(context)\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": refined_response})\n",
    "            print(\"_\" * 50)\n",
    "            print(\"IA: \" + refined_response)\n",
    "        else:\n",
    "            # Si no hay respuesta en Chroma o la similitud es baja, no usar el conocimiento previo\n",
    "            print(\"Lo siento, no encontré suficiente información en los documentos proporcionados para responder a tu pregunta.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805b2f44-5994-4262-a50c-2037f1657c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG System\n",
      "Number of existing ids in db: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chat()\n",
      "Cell \u001b[1;32mIn[1], line 104\u001b[0m, in \u001b[0;36mchat\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m():\n\u001b[1;32m--> 104\u001b[0m     rag_system \u001b[38;5;241m=\u001b[39m RAGSystem()\n\u001b[0;32m    105\u001b[0m     chat_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    106\u001b[0m     umbral_similitud \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Ajusta este umbral según sea necesario\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m, in \u001b[0;36mRAGSystem.__init__\u001b[1;34m(self, data_dir_path, db_path)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m                        Eres una IA llamada Espesito que se centra en responder preguntas pregunta sobre la\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124m                        Universidad de las Fuerzas Armadas ESPE, debes dar respuestas simples a las preguntas y siempre que puedas usa \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m                        Answer:\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m                        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_collection()\n",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m, in \u001b[0;36mRAGSystem._setup_collection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m chunks_to_add \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m chunks \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ids_in_db]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunks_to_add) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     vectordb\u001b[38;5;241m.\u001b[39madd_documents(chunks_to_add, ids\u001b[38;5;241m=\u001b[39m[i\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m chunks_to_add])\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded to db: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks_to_add)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m     vectordb\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\vectorstores\\base.py:469\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    468\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\vectorstores\\chroma.py:276\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\embeddings\\ollama.py:211\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m instruction_pairs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m--> 211\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(instruction_pairs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\embeddings\\ollama.py:199\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_emb_response(prompt) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\embeddings\\ollama.py:199\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_emb_response(prompt) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\embeddings\\ollama.py:164\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    158\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    161\u001b[0m }\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     res \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    166\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    167\u001b[0m         json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params},\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1385\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1386\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36df5e-6a1c-42c0-8999-74f848977497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
