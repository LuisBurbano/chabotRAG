{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fc2b9a-6deb-4975-8eb8-7aa05069784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizado por Luis Burbano, Cesar Loor, Sebastian Torres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994a82d3-c11c-4ff0-9790-56bfba30c4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3afdde-66b3-43b1-b9c8-a707f636376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV con delimitador ;\n",
    "file_path = 'Base_conocimiento_pre.csv'  # Actualiza el path si es necesario\n",
    "df = pd.read_csv(file_path, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b3c647-7994-4324-8cf6-6eaf40c828d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar datos faltantes en la columna 'Pregunta'\n",
    "df['Pregunta'] = df['Pregunta'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32561570-2cd0-4703-af3f-4e48668884e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo de embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef52c3c-e4c6-49a0-900d-f6eddd601366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear embeddings para las preguntas\n",
    "df['question_embeddings'] = df['Pregunta'].apply(lambda x: model.encode(x).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a320cb-abf7-42ad-a842-5b485b6d4bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d8cc6c-7a19-4189-a7fe-55fbd620c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una colecci√≥n\n",
    "collection = client.create_collection(name=\"qa_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "833225ec-1df5-492a-a69f-86f398e562f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar vectores a la colecci√≥n\n",
    "for index, row in df.iterrows():\n",
    "    collection.add(\n",
    "        documents=[row['Pregunta']],\n",
    "        embeddings=[row['question_embeddings']],\n",
    "        ids=[str(index)]  # Cambiado metadatas a ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3a1af2-b7c8-47cb-b24a-7c1139b8cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "    \"\"\"\n",
    "    Obtiene la mejor respuesta para una pregunta dada usando la colecci√≥n y el modelo de embeddings.\n",
    "\n",
    "    Args:\n",
    "    - pregunta (str): La pregunta para la cual obtener una respuesta.\n",
    "    - collection: La colecci√≥n de documentos en Chroma.\n",
    "    - df: El DataFrame que contiene las preguntas y respuestas.\n",
    "    - model: El modelo de embeddings para generar la representaci√≥n de la pregunta.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[str, float]: La mejor respuesta correspondiente a la pregunta y la distancia de similitud.\n",
    "    \"\"\"\n",
    "    # Codificar la pregunta\n",
    "    query_embedding = model.encode(pregunta).tolist()\n",
    "\n",
    "    # Realizar la consulta en la colecci√≥n\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=1)\n",
    "\n",
    "    # Obtener las listas de IDs y distancias\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:  # No hay resultados\n",
    "        return None, float('inf')\n",
    "\n",
    "    # Combina los IDs y distancias en un solo objeto\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "\n",
    "    # Ordena los resultados por distancia (menor distancia significa mayor similitud)\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "\n",
    "    # Selecciona solo el mejor resultado (menor distancia)\n",
    "    best_result = sorted_results[0]  # Solo el primer resultado es el mejor\n",
    "\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "\n",
    "    # Obtener la respuesta desde el DataFrame\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "\n",
    "    return respuesta, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295919ec-2f8a-43b8-a07d-d8d791ac2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del modelo Llama3\n",
    "llm = Ollama(model=\"gemma2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d228051e-1ee8-4c3e-bf4c-30d97c562641",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", bas√°ndote en la informaci√≥n proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c95726-f542-4d53-9eea-4f77c7e4a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "857e03c4-1099-4827-9ebe-bd974e04e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5  # Ajusta este umbral seg√∫n sea necesario\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            return\n",
    "        \n",
    "        # Primero, intenta obtener una respuesta de Chroma\n",
    "        answer, distance = obtener_respuesta(pregunta, collection, df, model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            response = answer\n",
    "        else:\n",
    "            # Si no hay respuesta en Chroma o la similitud es baja, utiliza el modelo Llama3\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "        \n",
    "        chat_history.append(HumanMessage(content=pregunta))\n",
    "        chat_history.append(AIMessage(content=response))\n",
    "        print(\"_\" * 50)\n",
    "        print(\"IA: \" + response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3ae68f3-4495-47c0-9b4c-600763ab8257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hola!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "IA: ¬°Hola! üëã ¬øEn qu√© puedo ayudarte hoy sobre la Universidad de las Fuerzas Armadas ESPE? üòä \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Quiero estudiar una carrera enfocada al desarrollo web, ¬øcual me recomiendas?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "IA: ¬°Excelente pregunta! Para el desarrollo web en ESPE, te recomendar√≠a que revises la carrera de **Ingenier√≠a en Sistemas Inform√°ticos**.  Esta carrera te proporciona las bases en programaci√≥n, dise√±o web y estructuras de datos que necesitas para un trabajo exitoso en desarrollo web. \n",
      "\n",
      "¬øTe gustar√≠a saber m√°s sobre esta carrera o hay algo espec√≠fico del desarrollo web que te interese? üòä\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  ¬øLa ESPE es una universidad gratuita?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "IA: la universidad dispone de 37 carreras  se divide en linea y presencial de las cuales son las siguientes \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:   Si estoy realizando un cambio de universidad y la carrera a la que quiero ir es Software, ¬øcon quien debo hablar?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "IA: Puedes empezar por contactar con el Director de la Carrera que deseas postular, el/la director@ te proporcionara informacion sobre los requisitos y condiciones. \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  ¬øQuien es coordinador de pasant√≠as del DCCO?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "IA: Phd. Edison Jorge Lascano \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Si necesito hacer pasant√≠as y soy del DCCO, ¬øcon qui√©n debo hablar?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "IA: Para hacer pasant√≠as en el DCCO, deber√≠as comunicarte con el **Phd. Edison Jorge Lascano**,  el coordinador de pasant√≠as. üòä\n",
      "\n",
      "¬øHay algo m√°s que te gustar√≠a saber sobre las pasant√≠as? \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  adios\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78acbc0b-1075-4887-977d-e0cc2d9babca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuestas cargadas: ['En el bloque H\\xa0', 'En bienestar estudiantil, junto al bar\\xa0', 'Una vez al a√±o', 'Dos accesos  para vehiculos y peatones', 'ASOCIACI√ìN DE INGENIEROS MEC√ÅNICOS ‚ÄúASME‚Äù\\r\\nCHACAREROS IASA\\r\\nROB√ìTICA\\r\\nRAMA ESTUDIANTIL IEEE\\r\\nPOL√çTICA EXTERIOR Y DIPLOMACIA\\r\\nGRUPO ESTUDIANTIL (Earthquake Engineering Research Institute) ‚ÄúEERI-ESPE‚Äù\\r\\nECOL√ìGICO\\r\\nDESARROLLO DE SOFTWARE\\r\\nFOTOGRAF√çA\\r\\nDANZA\\r\\nM√öSICA (Ensamble)\\r\\nORATORIA\\r\\nLIDERAZGO', 'Seg√∫n el articulo 5 de la LOES, los derechos de los estudiantes son:\\r\\n\\r\\na) Acceder, movilizarse, permanecer, egresar y titularse sin discriminaci√≥n conforme sus m√©ritos acad√©micos.\\r\\n\\r\\nb) Acceder a una educaci√≥n superior de calidad y pertinente, que permita iniciar una carrera acad√©mica y/o profesional en igualdad de oportunidades.\\r\\n\\r\\nc) Contar y acceder a los medios y recursos adecuados para su formaci√≥n superior; garantizados por la Constituci√≥n.\\r\\n\\r\\nd) Participar en el proceso de evaluaci√≥n y acreditaci√≥n de su carrera.\\r\\n\\r\\ne) Elegir y ser elegido para las representaciones estudiantiles e integrar el cogobierno, en el caso de las instituciones de educaci√≥n superior.\\r\\n\\r\\nf) Ejercer la libertad de asociarse, expresarse y completar su formaci√≥n bajo la m√°s amplia libertad de c√°tedra e investigativa.\\r\\n\\r\\ng) Participar en el proceso de construcci√≥n, difusi√≥n y aplicaci√≥n del conocimiento.\\r\\n\\r\\nh) El derecho a recibir una educaci√≥n superior laica, intercultural, democr√°tica, incluyente y diversa, que impulse la equidad de g√©nero, la justicia y la paz.\\r\\n\\r\\ni) Obtener de acuerdo con sus m√©ritos acad√©micos becas, cr√©ditos y otras formas de apoyo econ√≥mico que le garantice igualdad de oportunidades en el proceso de formaci√≥n de educaci√≥n superior.\\r\\n\\r\\nj) A desarrollarse en un √°mbito educativo libre de todo tipo de violencia.', 'Son aquellos matriculados en el 60% de las asignaturas, cursos o sus equivalentes que permite su estudio en el periodo acad√©mico ordinarios original.', 'Seg√∫n el articulo 11 del reglamento para garantizar el cumplimiento de gratuidad de educaci√≥n superior p√∫blica, preescribe que se pierde la gratuidad de estudio p√∫blico al perder el 30% de las horas correspondidas a las asigaturas.', 'En el art√≠culo 77, establece \"Las instituciones de educaci√≥n superior establecer√°n programas de becas completas o su equivalente en ayudas econ√≥micas que apoyen en su escolaridad a por lo menos el 10% del n√∫mero de estudiantes regulares. Ser√°n beneficiarios quienes no cuenten con recursos econ√≥micos suficientes, los estudiantes regulares con alto promedio y distinci√≥n acad√©mica, los deportistas de alto rendimiento que representen al pa√≠s en eventos interna ciona les, a condici√≥n de que acrediten niveles de rendimiento acad√©mico regulados por cada instituci√≥n y los discapacitados.\"', 'En el art√≠culo 86 indica la s funciones de Unidad de Bienestar Estudiantil.- \"Las instituciones de educaci√≥n superior mantendr√°n una unidad administrativa de Bienestar Estudiantil destinada a promover la orientaci√≥n vocacional y profesional, facilitar la obtenci√≥n de cr√©ditos, est√≠mulos, ayudas econ√≥micas y becas, y ofrecer los servicios asistenciales que se determinen en las normativas de cada instituci√≥n. Esta unidad,\\r\\nadem√°s, se encargar√° de promover un ambiente de respeto a los derechos y a la integridad f√≠sica, psicol√≥gica y sexual de las y los estudiantes, en un ambiente libre de violencia, y brindar√° asistencia a quienes demanden por violaciones de estos derechos.', 'Para reservar un aula que no se encuentra en uso, puedes hacerlo directamente con los encargados de los diferentes laboratorios', 'Puedes acercate directamente a las oficias de TICS que se encuentra en el bloque A, ellos te pueden ayudar con informaci√≥n de la plataforma y mucho m√°s.', 'Actualmente hay 2 puntos de pago y recarga de tarjetas para el parqueadero, uno de ellos se encuentra en el Bloque C segundo piso, y otro cerca de la biblioteca.', 'Si es problemas por almacenamiento en tu cuenta, trata de borrar archivos que no te sirven con ello en unos d√≠as ya podr√°s hacer uso de tu correo, sino puedes ir al edificio de TICS para explicar tu problema, tambien tienes a disposici√≥n el Help Desk', 'Podr√≠as mandar un correo al departamento de TICS informando de tu problema, pero habitualmente el tiempo que demora en activar las aulas virtuales para los estudiantes que recien realizan el pago de su matricula es de 5 d√≠as aproximadamente.', 'hay 22 carreras presenciales (nota: hay que a√±adir las carreras y la sede)', '$36.20', 'Ing. Mauricio Camapa√±a Alias \"Monster\"', 'la universidad dispone de 37 carreras  se divide en linea y presencial de las cuales son las siguientes ', 'ya le agrego tengo averiguar los curso ', 'Presentar por escrito y digital una queja hacia tesoreria  ', 'No devuelven ', '1. Por una mala gesti√≥n de las personas encargas de las aulas virtuales (Utic ) \\r\\n2 . No se cancelaron los pagos correspondientes a perdida de materias (en el caso que este realizando 2da o 3ra matricula) ', '36\\xa0laboratorios', '2,\\xa0Bloque A\\xa0y\\xa0Bloque B', '102 a√±os', '6, Educaci√≥n B√°sica, Educaci√≥n Inicial, Economia, Turismo, Tecnolog√≠as de la Informaci√≥n, Pedagog√≠a de los Idiomas y Extranjeros', 'Al 2025, ser reconocidos a nivel nacional e internacional como una instituci√≥n de educaci√≥n superior de calidad en docencia, investigaci√≥n y vinculaci√≥n bajo el paradigma de una universidad inteligente, articulando la transferencia de ciencia y tecnolog√≠a, a trav√©s de procesos de I+D+i; y, convirti√©ndonos en un referente de pensamiento en seguridad y defensa, al servicio del pa√≠s y Fuerzas Armadas.', 'Para los estudiantes que una vez registrado su matr√≠cula desean retirarse de forma parcial o total del per√≠odo acad√©mico', 'Debes esperar hasta el tiempo de matr√≠cula extraordinaria para registrar tus NRCs.', 'Debes tomar contacto con tu Director de Carrera para que canalice tu necesidad de cupos ante el Coordinador de Docencia o a su vez te informe cuales est√°n disponible mismos que deben ser equivalentes.', 'Debes tomar contacto inmediato con la tesorer√≠a de la Universidad\\xa0 para que te informen el saldo pendiente que debes cancelar en la Universidad.', 'Es con quien se toma el primer contacto ante cualquier necesidad acad√©mica que tenga el estudiante e inconvenientes al momento de registrar tu matr√≠cula.', 'La biblioteca de la ESPE ofrece servicios de pr√©stamo de libros, consulta de materiales en sala, acceso a bases de datos electr√≥nicas, espacios de estudio, y apoyo en investigaci√≥n a trav√©s de asesor√≠as.', 'La informaci√≥n sobre intercambios estudiantiles se puede obtener en la Oficina de Relaciones Internacionales, ubicada en el bloque A, planta baja.', 'La cafeter√≠a de la ESPE est√° abierta de lunes a viernes de 7:00 a 18:00 y los s√°bados de 8:00 a 13:00.', 'Para acceder a los servicios m√©dicos en la ESPE, puedes dirigirte al Centro M√©dico Universitario, ubicado en el bloque B, planta baja. Los horarios de atenci√≥n son de lunes a viernes de 8:00 a 16:00.', 'La ESPE ofrece una variedad de actividades extracurriculares, incluyendo deportes, clubes culturales y acad√©micos, talleres de arte y m√∫sica, y programas de voluntariado. Puedes obtener m√°s informaci√≥n en la Direcci√≥n de Bienestar Estudiantil.', 'Si se trata de cambios dentro de la misma universidad, contacta con el DIrector de Carrera de la carrera a la que quieras solicitar cupo, el/la director@ te brindara mas informacion acerca de requisitos y condiciones del proceso.', 'Tienes dos alternativas. Quieres realizar un cambio hacia la ESPE o un cambio desde la ESPE hacia otra universidad?', 'Puedes empezar por contactar con el Director de la Carrera que deseas postular, el/la director@ te proporcionara informacion sobre los requisitos y condiciones. ', 'Puedes empezar por contactar con las Autoridades /Decanos/Directores de tu carrera o universidad de destino, ellos te brindaran informacion acerca del proceso. Por otra parte contacta a la autoridad de tu carrera actual para que te brinde una guia acerca de como realizar correctamente el proceso de salida de la ESPE.', 'El Director vigente de la carrera de Ing Mecatronica SEDE Matriz es el Ing. Pa√∫l Mej√≠a Campoverde, PhD. Si necesitas contactar con el Director de Ing Mecatronica, este es su correo institucional: carreramecatronica@espe.edu.ec ', 'El Director vigente de la carrera de Ing Mecatronica SEDE Latacunga es Dar√≠o Mendoza C., M√°ster. Si necesitas contactar con el Director de Ing Mecatronica, este es su correo institucional: djmendoza@espe.edu.ec', 'El Director vigente de la carrera de Ing Mecanica es el Ing. Jaime F. Echeverr√≠a Y. Si necesitas contactar con el Director de Ing Mecanica, este es su correo institucional: jfecheverria@espe.edu.ec', 'El Director vigente de la carrera de Ing en Tecnologias de la Informacion es el Ing. Fidel Castro, MSc. Si necesitas contactar con el Director de Ing en Tecnologias de la Informacion, este es su correo institucional: flcastro@espe.edu.ec', 'Si, la Universidad de las Fuerzas Armadas cuenta con residencia universitaria, para obtener mas informaci√≥n puedes contactarte a los siguientes tel√©fonos: tel√©fonos: (02) 3989400 extensiones: 3125 y 3126, (02) 3989476 ', 'Si, la Universidad de las Fuerzas Armadas presta el servicio de gimnasio, est√° ubicado en el coliseo Grad. Miguel Iturralde en la Sede de Sangolqu√≠.', 'El gimnasio cuenta con horarios de lunes a viernes desde las 7:00 hasta las 13:00 en la ma√±ana y en la tarde desde las 15:00 hasta las 18:30.', 'Para entrar al gimnasio es obligatorio presentar tu carn√©t de la ESPE y traer toalla personal, alcohol antis√©ptico y ropa deportiva.', 'Al momento el servicio de las maquinas dispensadoras se encuentran bajo los servicios de la marca Maincoffee, si quieres contactarlos para resolver alg√∫n problema puedes llamarlos al 0997653889', 'La empresa a cargo del parqueadero es ESPE-Innovativa, los puedes contactar al tel√©fono (593-2) 3820800, al correo electr√≥nico info@espe-innovativa.edu.ec o en sus oficinas ubicadas en Av. General Rumi√±ahui s/n,Campus Universitario ESPE. Edificio de Servicios Generales ESPE, tras el Almac√©n Universitario.', 'El estacionamiento es gratis para los usuarios que no excedan los 15 minutos de estacionamiento.', 'La Universidad de las Fuerzas Armadas ESPE cuenta con los Campus: Campus Matriz, Campus Latacunga y Campus Santo Domingo', 'Debe comunicarse con la Unidad de Talento Humano para postular a un empleo', 'La Universidad cuenta con los departamentos: Ciencias Exactas, Ciencias de la Computaci√≥n, Ciencias de la Vida\\r\\nCiencias Econ√≥micas Administrativas y de Comercio. El√©ctrica y Electr√≥nica, Energ√≠a y Mec√°nica, Seguridad y Defensa, Ciencias de la Tierra y de la Construcci√≥n, Ciencias Humanas y Sociales', 'El n√∫mero de contacto del Campus Matriz es (593)23989-400', 'Puede obtener informaci√≥n del curso de Nivelaci√≥n en la Unidad de Admisi√≥n y Registro, en el apartado de Nivelaci√≥n', 'La Universidad ofrece la modalidad presencial, virtual y semipresencial', 'La Universidad ofrece Becas de Grado, Becas de Maestr√≠a y Becas de Doctorado', 'La Universidad cuenta con los idiomas de Ingl√©s y Chino Mandar√≠n', 'El aprendizaje del Idioma Ingl√©s cuenta con 6 niveles obligatorios y 2 opcionales', 'La Universidad de las Fuerzas Armadas ESPE tiene su origen el 16 de junio de 1922', 'El horario de atenci√≥n de la Universidad es de Lunes a Viernes de 07h00 a 16h00', 'Los estudiantes deber√°n registrarse para rendir el examen de ubicaci√≥n, en el formulario que se habilitar√° en la Plataforma Mi ESPE ', 'Debe comunicarse al n√∫mero (593) 23989 400', 'La carrera de Biotecnolog√≠a tiene una duraci√≥n de 9 semestres', 'La carrera de Medicina tiene una duraci√≥n de 10 semestres', 'Acercarse de manera presencial donde el Director de su Carrera y entregar la aceptaci√≥n de matr√≠cula para primer nivel, el documento\\xa0 debe ser \\r\\ndescargado, llenado y firmado por el estudiante', 'Una vez culminado un periodo acad√©mico, posterior a la finalizaci√≥n del periodo normal de estudios, pasado este tiempo el estudiante tendr√° que pedir una pr√≥rroga de un periodo acad√©mico ordinario al Director del Centro de Posgrados, esta primera pr√≥rroga es gratuita. El estudiante tendr√° derecho de solicitar una segunda pr√≥rroga con pago para la extensi√≥n de un plazo adicional de un periodo acad√©mico ordinario.', 'El tiempo de ejecuci√≥n de los proyectos de vinculaci√≥n con la sociedad, estar√° acorde a las actividades planificadas. La eiecuci√≥n de un proyecto no podr√° ser menor a tres meses.', 'Se puede comenzar desde el primer nivel de ingles o realizar el examen de ubicacion  con la finalidad de ubicar al estudiante de acuerdo al nivel de conocimiento.', 'Si se puede realizar por segunda ocsasion el examen de ubicaci√≥n pero a partir del segundo examen deber√° cancelar el valor correspondiente al 100%.', 'Los estudiantes de carreras que no han dado continuidad en los cursos de idiomas por el lapso de dos per√≠odos acad√©micos, deber√°n registrarse para rendir el examen de ubicaci√≥n, en el formulario que se habilitar√° en la Plataforma Mi ESPE ; caso contrario deber√°n matricularse en ENGLISH I (primer nivel) en las fechas establecidas.', 'Como parte de la formaci√≥n academica para realizar la titulaci√≥n de una carrera se debe cumplir 240 horas', 'No existen materias optativas en la Universidad de las Fuerzas Armadas ESPE', 'Es un proyecto investigativo que permite la titulaci√≥n del estudiante de una carrera, esta materia se encuentra en los ultimos niveles de la malla de cualquier carrera', 'No, la universidad esta habilitada para personal civil y personal militar', 'El maximo de tercera matriculas que puede hacer un estudiante es de 3', 'Como pare de la formaci√≥n academica para realizar la titulaci√≥n de una carrera se deben cumplir 96 horas', 'No, el nuevo hospital universitario PADIS-ESPE esta ubicado en el valle de los chillos pero no en el campus matriz', 'S√≠, son cursos que corresponden al centro de educaci√≥n continua y tienen para la areas de ciencias exactas, m√©dicas, energ√≠a y mec√°nica, de la vida y agricultura, computaci√≥n, el√©trica, eletr√≥nica y telecomunicaciones, econ√≥micas, administrativas y de comercio, humanas y sociales, de la tierra y construcci√≥n, seguridad y defensa', 'No, solo pueden cursar estas carreras personal militar', 'En el caso que se presente esta situaci√≥n , se debe comunicar al problema al director de carrera √©l cual dar√° una resoluci√≥n para poder matricularte sin que presente incovenientes en la matricula como perdida de gratuitida.', 'Av. 6 de Diciembre y Thom√°s de Berlanga  ‚Äì Campus el Inca', 'Los cr√©ditos m√≠nimos para no perder gratuidad de matr√≠cula es el ', 'Phd. Edison Jorge Lascano ', 'S√≠, todas las carreras de ingenier√≠a cuentan con un 30% de la malla de materias exactas.', 'Phd. Mgstr Sonia Cardenas', 'Los requisitos suelen incluir tener un alto promedio acad√©mico o mayor a 16, no tener recursos econ√≥micos suficientes, y cumplir con los criterios establecidos por la universidad. Para m√°s detalles espec√≠ficos, debes contactar con el departamento de becas de la universidad.', 'La ESPE ofrece varias facilidades deportivas, incluyendo canchas de f√∫tbol, baloncesto, voleibol, y un gimnasio. Los horarios y disponibilidad de estas instalaciones pueden ser consultados en la Direcci√≥n de Bienestar Estudiantil o en la administraci√≥n de las instalaciones deportivas.', 'La ESPE organiza varias actividades sociales y recreativas para nuevos estudiantes, como jornadas de integraci√≥n, eventos deportivos, y actividades culturales. Puedes encontrar m√°s informaci√≥n en el calendario acad√©mico o en la Direcci√≥n de Bienestar Estudiantil.', 'La Universidad de las Fuerzas Armadas ESPE se origin√≥ el 16 de junio de 1922 como la Escuela de Oficiales de Ingenieros, orientada a la formaci√≥n de oficiales del Ej√©rcito en t√©cnicas de ingenier√≠a militar. En 1977, se transform√≥ en la Escuela Polit√©cnica del Ej√©rcito (ESPE), y en 2013, se fusion√≥ con otras instituciones militares para convertirse en la Universidad de las Fuerzas Armadas ESPE, como se la conoce hoy en d√≠a\\u200b (Wikipedia, la enciclopedia libre)\\u200b\\u200b (ESPE)\\u200b.', '¬°S√≠! La ESPE no solo se dedica a la educaci√≥n en aulas, sino que tambi√©n cuenta con una granja experimental en el campus de Sangolqu√≠. Aqu√≠, los estudiantes de carreras relacionadas con ciencias agropecuarias pueden practicar y aplicar sus conocimientos', 'Burbano uwu y mero uwu']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceEmbeddings\nmodel_name\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 71\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Uso del chatbot con gemma2 a trav√©s de Ollama\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m ChatbotModel()\n\u001b[0;32m     72\u001b[0m respuesta \u001b[38;5;241m=\u001b[39m chatbot\u001b[38;5;241m.\u001b[39mget_response_from_first_bot(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m¬øD√≥nde se encuentra la biblioteca principal?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(respuesta)\n",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m, in \u001b[0;36mChatbotModel.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Crear el vectorstore de Chroma\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore \u001b[38;5;241m=\u001b[39m Chroma(embedding_function\u001b[38;5;241m=\u001b[39mSentenceTransformerEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel), collection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:203\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     emit_warning()\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\embeddings\\huggingface.py:69\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the sentence_transformer.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceEmbeddings\nmodel_name\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "class ChatbotModel:\n",
    "    def __init__(self):\n",
    "        self.llm = Ollama(model=\"gemma2\")\n",
    "        self.model = SentenceTransformer('fine-tuned/jinaai_jina-embeddings-v2-base-es-2572024-mb4o-webapp', trust_remote_code=True)\n",
    "\n",
    "\n",
    "        # Cargar respuestas y generar embeddings\n",
    "        df = pd.read_csv('Base_conocimiento_pre.csv', delimiter=';')\n",
    "        \n",
    "        # Filtrar respuestas que no sean NaN y convertir todas las respuestas a cadenas\n",
    "        self.responses = df['Respuesta'].dropna().astype(str).tolist()\n",
    "        \n",
    "        # Verifica que todas las respuestas sean cadenas de texto\n",
    "        print(f\"Respuestas cargadas: {self.responses}\")\n",
    "\n",
    "        # Generar embeddings de las respuestas\n",
    "        self.response_embeddings = self.model.encode(self.responses)\n",
    "\n",
    "        # Crear el vectorstore de Chroma\n",
    "        self.vectorstore = Chroma(embedding_function=SentenceTransformerEmbeddings(model_name=self.model), collection=self.collection)\n",
    "        \n",
    "        # Configuraci√≥n del modelo Llama3 con gemma2 usando Ollama\n",
    "        \n",
    "\n",
    "    def truncate_vector(self, vector, size=None):\n",
    "        # Truncar vector para mantener la dimensi√≥n esperada\n",
    "        size = size or self.vector_dimension\n",
    "        return vector[:size] if len(vector) > size else vector\n",
    "\n",
    "    def get_best_response_from_second_bot(self, query, threshold=0.5):\n",
    "        # Obtener la mejor respuesta del segundo bot (utilizando el modelo de embeddings)\n",
    "        query_embedding = self.model.encode([query])[0]\n",
    "        similarities = util.pytorch_cos_sim(query_embedding, self.response_embeddings)[0]\n",
    "        max_similarity = similarities.max().item()\n",
    "        best_response_idx = similarities.argmax().item()\n",
    "        if max_similarity < threshold:\n",
    "            return None\n",
    "        return self.responses[best_response_idx]\n",
    "\n",
    "    def get_response_from_first_bot(self, user_message):\n",
    "        # Buscar la respuesta en Chroma\n",
    "        user_embedding = self.model.encode([user_message])[0]\n",
    "        truncated_embedding = self.truncate_vector(user_embedding)\n",
    "        results = self.vectorstore.similarity_search(query_texts=[user_message])\n",
    "    \n",
    "        print(f\"Resultados obtenidos: {results}\")  # Depuraci√≥n\n",
    "    \n",
    "        # Asegurarse de que results sea una lista de diccionarios\n",
    "        if isinstance(results, list) and len(results) > 0 and isinstance(results[0], dict) and 'answer' in results[0]:\n",
    "            return results[0]['answer']\n",
    "    \n",
    "        # Si no hay resultados v√°lidos, generar respuesta con gemma2 usando Ollama\n",
    "        response = self.llm.generate_response(user_message)\n",
    "    \n",
    "        # Agregar la nueva respuesta a la base de datos de Chroma\n",
    "        self.vectorstore.add_texts(texts=[user_message], metadatas=[{'answer': response}])\n",
    "    \n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "# Uso del chatbot con gemma2 a trav√©s de Ollama\n",
    "chatbot = ChatbotModel()\n",
    "respuesta = chatbot.get_response_from_first_bot(\"¬øD√≥nde se encuentra la biblioteca principal?\")\n",
    "print(respuesta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ad2347-695b-452a-8ffd-671eea71efef",
   "metadata": {},
   "outputs": [
    {
     "ename": "UniqueConstraintError",
     "evalue": "Collection qa_collection already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUniqueConstraintError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m client \u001b[38;5;241m=\u001b[39m Client(settings\u001b[38;5;241m=\u001b[39msettings)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Crear una colecci√≥n\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m collection \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_collection(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Agregar vectores a la colecci√≥n\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\api\\client.py:112\u001b[0m, in \u001b[0;36mClient.create_collection\u001b[1;34m(self, name, metadata, embedding_function, data_loader, get_or_create)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_collection\u001b[39m(\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m     get_or_create: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    111\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[0;32m    113\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m    114\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    115\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[0;32m    116\u001b[0m         data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[0;32m    117\u001b[0m         tenant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant,\n\u001b[0;32m    118\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[0;32m    119\u001b[0m         get_or_create\u001b[38;5;241m=\u001b[39mget_or_create,\n\u001b[0;32m    120\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\api\\segment.py:171\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[1;34m(self, name, metadata, embedding_function, data_loader, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    167\u001b[0m check_index_name(name)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m uuid4()\n\u001b[1;32m--> 171\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sysdb\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m    173\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m    174\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    175\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    176\u001b[0m     get_or_create\u001b[38;5;241m=\u001b[39mget_or_create,\n\u001b[0;32m    177\u001b[0m     tenant\u001b[38;5;241m=\u001b[39mtenant,\n\u001b[0;32m    178\u001b[0m     database\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[0;32m    179\u001b[0m )\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# TODO: wrap sysdb call in try except and log error if it fails\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\db\\mixins\\sysdb.py:220\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[1;34m(self, id, name, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    214\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_collections(\n\u001b[0;32m    215\u001b[0m                 \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mcollection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase\n\u001b[0;32m    216\u001b[0m             )[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    217\u001b[0m             \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    218\u001b[0m         )\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UniqueConstraintError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m collection \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m    224\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    230\u001b[0m )\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtx() \u001b[38;5;28;01mas\u001b[39;00m cur:\n",
      "\u001b[1;31mUniqueConstraintError\u001b[0m: Collection qa_collection already exists"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Cargar el archivo CSV con delimitador ;\n",
    "file_path = 'Base_conocimiento_pre.csv'  # Actualiza el path si es necesario\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "# Limpiar datos faltantes en la columna 'Pregunta'\n",
    "df['Pregunta'] = df['Pregunta'].fillna('')\n",
    "\n",
    "# Cargar el modelo de embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Crear embeddings para las preguntas\n",
    "df['question_embeddings'] = df['Pregunta'].apply(lambda x: model.encode(x).tolist())\n",
    "\n",
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)\n",
    "\n",
    "# Crear una colecci√≥n\n",
    "collection = client.create_collection(name=\"qa_collection\")\n",
    "\n",
    "# Agregar vectores a la colecci√≥n\n",
    "for index, row in df.iterrows():\n",
    "    collection.add(\n",
    "        documents=[row['Pregunta']],\n",
    "        embeddings=[row['question_embeddings']],\n",
    "        ids=[str(index)]  # Cambiado metadatas a ids\n",
    "    )\n",
    "\n",
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "     # Codificar la pregunta\n",
    "    query_embedding = model.encode(pregunta).tolist()\n",
    "\n",
    "    # Realizar la consulta en la colecci√≥n\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=1)\n",
    "\n",
    "    # Obtener las listas de IDs y distancias\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:  # No hay resultados\n",
    "        return None, float('inf')\n",
    "\n",
    "    # Combina los IDs y distancias en un solo objeto\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "\n",
    "    # Ordena los resultados por distancia (menor distancia significa mayor similitud)\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "\n",
    "    # Selecciona solo el mejor resultado (menor distancia)\n",
    "    best_result = sorted_results[0]  # Solo el primer resultado es el mejor\n",
    "\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "\n",
    "    # Obtener la respuesta desde el DataFrame\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "\n",
    "    return respuesta, distance\n",
    "\n",
    "# Configuraci√≥n del modelo Llama3\n",
    "llm = Ollama(model=\"gemma2\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", bas√°ndote en la informaci√≥n proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5  # Ajusta este umbral seg√∫n sea necesario\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            return\n",
    "        \n",
    "        # Primero, intenta obtener una respuesta de Chroma\n",
    "        answer, distance = obtener_respuesta(pregunta, collection, df, model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            # Usa la respuesta recuperada para generar una respuesta m√°s elaborada con Gemma2\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}\"\n",
    "            response = chain.invoke(input=context, chat_history=chat_history)\n",
    "        else:\n",
    "            # Si no se encuentra una respuesta adecuada, usa el modelo Llama3 directamente\n",
    "            response = chain.run(input=pregunta, chat_history=chat_history)\n",
    "        \n",
    "        print(f\"Espesito: {response}\")\n",
    "        chat_history.append(HumanMessage(content=pregunta))\n",
    "        chat_history.append(AIMessage(content=response))\n",
    "\n",
    "# Ejecutar el chat\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f268f5-1fc2-47e4-8d9f-5e235d76cf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hola!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BasePromptTemplate.invoke() got an unexpected keyword argument 'chat_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 109\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEspesito: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Iniciar el chat\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m chat()\n",
      "Cell \u001b[1;32mIn[1], line 103\u001b[0m, in \u001b[0;36mchat\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m     chat_history\u001b[38;5;241m.\u001b[39mappend(AIMessage(content\u001b[38;5;241m=\u001b[39manswer))\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# Si no hay una respuesta adecuada en Chroma, usa el modelo Llama3\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mpregunta, chat_history\u001b[38;5;241m=\u001b[39mchat_history)\n\u001b[0;32m    104\u001b[0m     chat_history\u001b[38;5;241m.\u001b[39mappend(HumanMessage(content\u001b[38;5;241m=\u001b[39mpregunta))\n\u001b[0;32m    105\u001b[0m     chat_history\u001b[38;5;241m.\u001b[39mappend(AIMessage(content\u001b[38;5;241m=\u001b[39mresponse))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\runnables\\base.py:2497\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m   2494\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2495\u001b[0m )\n\u001b[0;32m   2496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2497\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2499\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[1;31mTypeError\u001b[0m: BasePromptTemplate.invoke() got an unexpected keyword argument 'chat_history'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Cargar el archivo CSV con delimitador ;\n",
    "file_path = 'Base_conocimiento_pre.csv'  # Actualiza el path si es necesario\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "# Limpiar datos faltantes en la columna 'Pregunta'\n",
    "df['Pregunta'] = df['Pregunta'].fillna('')\n",
    "\n",
    "# Cargar el modelo de embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Crear embeddings para las preguntas\n",
    "df['question_embeddings'] = df['Pregunta'].apply(lambda x: model.encode(x).tolist())\n",
    "\n",
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)\n",
    "\n",
    "# Crear una colecci√≥n\n",
    "collection = client.create_collection(name=\"qa_collection\")\n",
    "\n",
    "# Agregar vectores a la colecci√≥n\n",
    "for index, row in df.iterrows():\n",
    "    collection.add(\n",
    "        documents=[row['Pregunta']],\n",
    "        embeddings=[row['question_embeddings']],\n",
    "        ids=[str(index)]  # Cambiado metadatas a ids\n",
    "    )\n",
    "\n",
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "        # Codificar la pregunta\n",
    "    query_embedding = model.encode(pregunta).tolist()\n",
    "\n",
    "    # Realizar la consulta en la colecci√≥n\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=1)\n",
    "\n",
    "    # Obtener las listas de IDs y distancias\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:  # No hay resultados\n",
    "        return None, float('inf')\n",
    "\n",
    "    # Combina los IDs y distancias en un solo objeto\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "\n",
    "    # Ordena los resultados por distancia (menor distancia significa mayor similitud)\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "\n",
    "    # Selecciona solo el mejor resultado (menor distancia)\n",
    "    best_result = sorted_results[0]  # Solo el primer resultado es el mejor\n",
    "\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "\n",
    "    # Obtener la respuesta desde el DataFrame\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "\n",
    "    return respuesta, distance\n",
    "\n",
    "# Configuraci√≥n del modelo Llama3\n",
    "llm = Ollama(model=\"gemma2\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", bas√°ndote en la informaci√≥n proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5  # Ajusta este umbral seg√∫n sea necesario\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            return\n",
    "        \n",
    "        # Primero, intenta obtener una respuesta de Chroma\n",
    "        answer, distance = obtener_respuesta(pregunta, collection, df, model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            # Usa la respuesta recuperada para generar una respuesta m√°s elaborada con Gemma2\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}\"\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=answer))\n",
    "        else:\n",
    "            # Si no hay una respuesta adecuada en Chroma, usa el modelo Llama3\n",
    "            response = chain.invoke(input=pregunta, chat_history=chat_history)\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=response))\n",
    "            print(f\"Espesito: {response}\")\n",
    "\n",
    "# Iniciar el chat\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497d0dd8-1a03-4a65-9e52-889262f9b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5  # Ajusta este umbral seg√∫n sea necesario\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            return\n",
    "        \n",
    "        # Primero, intenta obtener una respuesta de Chroma\n",
    "        answer, distance = obtener_respuesta(pregunta, collection, df, model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            # Usa la respuesta recuperada para generar una respuesta m√°s elaborada con Gemma2\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}\"\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=answer))\n",
    "            print(f\"Espesito: {answer}\")\n",
    "        else:\n",
    "            # Si no hay una respuesta adecuada en Chroma, usa el modelo Llama3\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=response))\n",
    "            print(f\"Espesito: {response}\")\n",
    "\n",
    "# Iniciar el chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6860b0fb-4e2b-4728-bfbc-4be0d89f531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hola\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: ¬°Hola! üëã  Soy Espesito, tu asistente virtual para preguntas sobre la Universidad de las Fuerzas Armadas \"ESPE\". ¬øQu√© te gustar√≠a saber? üòä  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  ¬øQuien es coordinador de pasant√≠as del DCCO?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Phd. Edison Jorge Lascano \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  y sabes quien es el director de carrera de software?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Ing. Mauricio Camapa√±a Alias \"Monster\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Sabes donde puedo encontrarlo?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Como tu asistente virtual, no tengo acceso a informaci√≥n en tiempo real como direcciones o correos electr√≥nicos. \n",
      "\n",
      "Sin embargo, te recomiendo que revises la p√°gina web del DCCO de la ESPE, ya que all√≠ suelen publicar informaci√≥n sobre el personal y sus contactos.  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quiero aplicar a una beca, sabes cuando se aplica?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Como tu asistente virtual, no tengo acceso a fechas espec√≠ficas para las aplicaciones de becas. Te sugiero que consultes el sitio web oficial de la ESPE o te pongas en contacto con la oficina de becas directamente. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Y sabes cada cuanto tiempo se aplican estas becas?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Una vez al a√±o\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Y donde puedo ir a hablar de esto?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Puedes dirigirte a la oficina de Becas de la ESPE.  \n",
      "\n",
      "Te recomiendo que visites su p√°gina web oficial para encontrar informaci√≥n sobre sus horarios de atenci√≥n y c√≥mo contactarlos. üòä \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  adios\n"
     ]
    }
   ],
   "source": [
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb6a7ef-4e5c-43c2-aa5a-e93c8c89fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Cargar el archivo CSV con delimitador ;\n",
    "file_path = 'Base_conocimiento_pre.csv'  # Actualiza el path si es necesario\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "# Limpiar datos faltantes en la columna 'Pregunta'\n",
    "df['Pregunta'] = df['Pregunta'].fillna('')\n",
    "\n",
    "# Cargar el modelo de embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Crear embeddings para las preguntas\n",
    "df['question_embeddings'] = df['Pregunta'].apply(lambda x: model.encode(x).tolist())\n",
    "\n",
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)\n",
    "\n",
    "# Crear una colecci√≥n\n",
    "collection = client.create_collection(name=\"qa_collection\")\n",
    "\n",
    "# Agregar todos los documentos a la colecci√≥n de una sola vez\n",
    "documents = df['Pregunta'].tolist()\n",
    "embeddings = df['question_embeddings'].tolist()\n",
    "ids = df.index.astype(str).tolist()\n",
    "\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "# Hasta aqu√≠, todas las preguntas y respuestas se han cargado en Chroma.\n",
    "\n",
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "    query_embedding = model.encode(pregunta).tolist()\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=1)\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:\n",
    "        return None, float('inf')\n",
    "\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "    best_result = sorted_results[0]\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "\n",
    "    return respuesta, distance\n",
    "\n",
    "# Configuraci√≥n del modelo Llama3\n",
    "llm = Ollama(model=\"gemma2\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", bas√°ndote en la informaci√≥n proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            return\n",
    "        \n",
    "        answer, distance = obtener_respuesta(pregunta, collection, df, model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}\"\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=answer))\n",
    "            print(f\"Espesito: {answer}\")\n",
    "        else:\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=response))\n",
    "            print(f\"Espesito: {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c6c104-4a04-4a03-b36c-b66412020b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hola\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: ¬°Hola! Soy Espesito, tu asistente virtual para todo lo relacionado con la Universidad de las Fuerzas Armadas \"ESPE\". ¬øQu√© te gustar√≠a saber? üòä  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  ueria hacer las practicas preprofesional o pasantias pero no se con quien hablar porque soy del departamento DCCO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: ¬°Hola! Entiendo que quieres realizar pr√°cticas preprofesionales o pasant√≠as desde el departamento de DCCO. \n",
      "\n",
      "Para darte informaci√≥n m√°s precisa, ¬øpodr√≠as decirme qu√© tipo de pr√°ctica te interesa?  Por ejemplo, ¬øbuscas una pr√°ctica en desarrollo de software, an√°lisis de datos, seguridad inform√°tica...?\n",
      "\n",
      "Con m√°s detalles podr√© orientarte mejor sobre a qui√©n contactar en ESPE para tu pr√°ctica preprofesional. üòä\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  pero quien es el coordinador\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: ¬°Claro que s√≠! Para pr√°cticas preprofesionales del departamento DCCO, puedes contactarte con **el profesor [Nombre del Coordinador]**, quien es el responsable de coordinar estas experiencias. \n",
      "\n",
      "\n",
      "Para obtener su correo electr√≥nico o n√∫mero telef√≥nico exacto, te recomiendo consultar la p√°gina web del Departamento de Ciencias y Computaci√≥n o comunicarte con la oficina administrativa de DCCO. ¬°Espero que esto te ayude! üòä\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  adios\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7797f08-c966-463e-a88a-d5bddf6ade27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "    query_embedding = model.encode(pregunta).tolist()\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=3)  # Recuperar m√°s resultados\n",
    "\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:\n",
    "        return None, float('inf')\n",
    "\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "    best_result = sorted_results[0]\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "\n",
    "    # Recuperar respuestas adicionales para proporcionar contexto\n",
    "    additional_context = [df.loc[int(doc_id), 'Respuesta'] for doc_id, _ in sorted_results[1:3]]\n",
    "\n",
    "    return respuesta, distance, additional_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2c1a65-07ce-4751-b708-fec6f1a35209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            return\n",
    "        \n",
    "        answer, distance, additional_context = obtener_respuesta(pregunta, collection, df, model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}. Informaci√≥n adicional: {' '.join(additional_context)}\"\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=answer))\n",
    "            print(f\"Espesito: {answer}\")\n",
    "        else:\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=response))\n",
    "            print(f\"Espesito: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4771d3-94a3-4e61-aea9-579d8e198b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hola\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: ¬°Hola! Bienvenido a ESPE. ¬øEn qu√© puedo ayudarte hoy? üòÑ \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  queria hacer las pasantias pero no se con quien hablar porque soy del departamento DCCO y tampoco se el bloque al que debo ir\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: ¬°Entiendo tu situaci√≥n!  Como estudiante del DCCO, para informaci√≥n sobre pasant√≠as debes dirigirte a la oficina del **Departamento de Pr√°cticas Universitarias**.  \n",
      "\n",
      "¬øTe gustar√≠a que te ayude a encontrar la ubicaci√≥n o los datos de contacto de esa oficina? üòä \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quien es el coordinador\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Para saber qui√©n es el coordinador del Departamento de Pr√°cticas Universitarias, te recomiendo que revises la p√°gina web oficial de ESPE o contactes directamente con ellos.  ¬°All√≠ podr√°n proporcionarte esa informaci√≥n! üòâ \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  me refiero a quien es el coordinador de las pasantias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Para saber qui√©n es el coordinador espec√≠fico de las pasant√≠as dentro del Departamento de Pr√°cticas Universitarias, lo mejor ser√≠a contactar directamente con ellos.  \n",
      "\n",
      "¬øTe gustar√≠a que te ayude a encontrar su informaci√≥n de contacto? üòä \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  pero el coordinador del DCCO de las pasantias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Phd. Edison Jorge Lascano \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  y me dijieron que una vez acabado las pasantias debo ir con el director de la carrera, sabes quien es\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Ing. Mauricio Camapa√±a Alias \"Monster\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Y sabes donde se encuentra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Como modelo de lenguaje, no tengo acceso a informaci√≥n en tiempo real como ubicaciones dentro de la universidad. \n",
      "\n",
      "Para encontrar la ubicaci√≥n del director de tu carrera, te sugiero que consultes el directorio de ESPE en su p√°gina web o preguntes en la oficina administrativa del DCCO. üòä  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  EN que bloque sabe encontrarse?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: ¬°Entiendo! Te recomiendo que contactes con el departamento administrativo del DCCO para obtener informaci√≥n precisa sobre el bloque donde se encuentra el director. üòä \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  y donde esta el departamento administrativo del DCCO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Phd. Edison Jorge Lascano \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  me refiero al bloque\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: ¬°Ah!  Entiendo mejor ahora. Para saber en qu√© bloque est√° ubicado el departamento administrativo del DCCO, te sugiero que consultes la p√°gina web oficial de ESPE o contactes con la oficina de atenci√≥n al estudiante. üòä \n",
      "\n",
      " Ellos podr√°n proporcionarte esa informaci√≥n. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  donde esta el departamento de computacion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: En el bloque H¬†\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  y ahi se encuentra el coordinador de pasantias? Ya que el departamento es el DCCO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Phd. Edison Jorge Lascano \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Osea si se encuentra ahi el coordinador?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Si, seg√∫n la informaci√≥n que has proporcionado, el coordinador de pasant√≠as del DCCO,  Phd. Edison Jorge Lascano, trabaja en el departamento de Computaci√≥n ubicado en el bloque H.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  y el director tambiem?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: El director de tu carrera se llama Ing. Mauricio Camapa√±a Alias \"Monster\", pero no puedo decirte en qu√© bloque est√°. Te recomiendo que contactes con la oficina administrativa del DCCO o consultes el directorio de ESPE en su p√°gina web. üòä \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  adios\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9179ae96-75aa-4db1-8890-c15e64b24aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Cargar el archivo CSV con delimitador ;\n",
    "file_path = 'Base_conocimiento_pre.csv'  # Actualiza el path si es necesario\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "df['Pregunta'] = df['Pregunta'].fillna('')\n",
    "\n",
    "# Cargar el modelo de embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Crear embeddings para las preguntas\n",
    "df['question_embeddings'] = df['Pregunta'].apply(lambda x: model.encode(x).tolist())\n",
    "\n",
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)\n",
    "\n",
    "# Crear una colecci√≥n\n",
    "collection = client.create_collection(name=\"qa_collection\")\n",
    "\n",
    "# Agregar todos los documentos a la colecci√≥n de una sola vez\n",
    "documents = df['Pregunta'].tolist()\n",
    "embeddings = df['question_embeddings'].tolist()\n",
    "ids = df.index.astype(str).tolist()\n",
    "\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "    query_embedding = model.encode(pregunta).tolist()\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=3)\n",
    "\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:\n",
    "        return None, float('inf')\n",
    "\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "    best_result = sorted_results[0]\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "\n",
    "    additional_context = [df.loc[int(doc_id), 'Respuesta'] for doc_id, _ in sorted_results[1:3]]\n",
    "\n",
    "    return respuesta, distance, additional_context\n",
    "\n",
    "# Configuraci√≥n del modelo Ollama\n",
    "llm = Ollama(model=\"gemma2\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", bas√°ndote en la informaci√≥n proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            break\n",
    "        \n",
    "        answer, distance, additional_context = obtener_respuesta(pregunta, collection, df, model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}. Informaci√≥n adicional: {' '.join(additional_context)}\"\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=answer))\n",
    "            print(f\"Espesito: {answer}\")\n",
    "        else:\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=response))\n",
    "            print(f\"Espesito: {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc78c52-8225-4044-b9fb-e5b81ac6370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hola\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: ¬°Hola! üëã Soy Espesito, tu asistente informativo sobre la Universidad de las Fuerzas Armadas \"ESPE\". ¬øTienes alguna pregunta para m√≠? üòä  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  ueria hacer las pasantias y como soy del departamento del DCCO queria saber quien es el coordinador \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Phd. Edison Jorge Lascano \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  y sabes donde se encuentra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: El Dr. Edison Jorge Lascano se ubica en la Facultad de Ingenier√≠a e Inform√°tica, dentro del Departamento de Ciencias Computaci√≥n (DCCO). \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  y sabes en donde esta este bloque\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: El Bloque de Ciencias Computaci√≥n (DCCO) se encuentra en el campus norte de la ESPE.  \n",
      "\n",
      "¬øNecesitas m√°s informaci√≥n sobre el campus o c√≥mo llegar al DCCO? üó∫Ô∏è \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Y cual es el bloque\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: El bloque del DCCO se llama **Bloque C**.  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  adios\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb9fb3a-5ffb-4f8a-81bb-58de7bae84fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch  # Aseg√∫rate de importar torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document  # Importa la clase Document\n",
    "\n",
    "# Cargar el archivo CSV con delimitador ;\n",
    "file_path = 'Base_conocimiento_pre.csv'  # Actualiza el path si es necesario\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "df['Pregunta'] = df['Pregunta'].fillna('')\n",
    "\n",
    "# Dividir los datos en fragmentos si es necesario\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = [Document(page_content=row['Pregunta']) for _, row in df.iterrows()]  # Crea objetos Document\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Crear embeddings para las preguntas\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "\n",
    "try:\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "except Exception as ex:\n",
    "    print(\"Exception: \", ex)\n",
    "    local_model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    print(f\"Use alternative (local) model: {local_model_path}\\n\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)\n",
    "\n",
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)\n",
    "\n",
    "# Crear una colecci√≥n\n",
    "collection = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")\n",
    "\n",
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "    query_embedding = model.encode(pregunta).tolist()\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=3)\n",
    "\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:\n",
    "        return None, float('inf')\n",
    "\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "    best_result = sorted_results[0]\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "\n",
    "    additional_context = [df.loc[int(doc_id), 'Respuesta'] for doc_id, _ in sorted_results[1:3]]\n",
    "\n",
    "    return respuesta, distance, additional_context\n",
    "\n",
    "# Configuraci√≥n del modelo Ollama\n",
    "llm = Ollama(model=\"gemma2\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", bas√°ndote en la informaci√≥n proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            break\n",
    "        \n",
    "        answer, distance, additional_context = obtener_respuesta(pregunta, collection, df, embeddings)  # Aseg√∫rate de pasar 'embeddings' en lugar de 'model'\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}. Informaci√≥n adicional: {' '.join(additional_context)}\"\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=answer))\n",
    "            print(f\"Espesito: {answer}\")\n",
    "        else:\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=response))\n",
    "            print(f\"Espesito: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f85631-8469-405e-976f-1ea25773a047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liak\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hola\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: ¬°Hola! üëã Soy Espesito, tu asistente virtual para todas tus dudas sobre la Universidad de las Fuerzas Armadas \"ESPE\". ¬øQu√© te gustar√≠a saber? üòä  \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  iba a realizar las pasantias y queria hablar con el coordinador, sabes como se llama porque pertencezco al departamento DCCO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Phd. Edison Jorge Lascano \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  y sabes en que edificio esta el departamento del DCCO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: Phd. Edison Jorge Lascano \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  me refiero al bloque\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espesito: El Departamento de Ciencia e Ingenier√≠a Computacional (DCCO) se encuentra en el **edificio A**.  \n",
      "\n",
      "\n",
      "Let me know if you have any other questions! üòä \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  adios\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Cargar el archivo CSV con delimitador ;\n",
    "file_path = 'Base_conocimiento_pre.csv'\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "df['Pregunta'] = df['Pregunta'].fillna('')\n",
    "\n",
    "# Dividir los datos en fragmentos si es necesario\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = [Document(page_content=row['Pregunta']) for _, row in df.iterrows()]\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Crear embeddings para las preguntas\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "\n",
    "try:\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "except Exception as ex:\n",
    "    print(\"Exception: \", ex)\n",
    "    local_model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    print(f\"Use alternative (local) model: {local_model_path}\\n\")\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)\n",
    "\n",
    "# Configurar Chroma\n",
    "settings = Settings()\n",
    "client = Client(settings=settings)\n",
    "collection = client.create_collection(name=\"qa_collection\")\n",
    "\n",
    "# Generar embeddings para todos los fragmentos\n",
    "all_embeddings = [embeddings_model.embed_documents([doc.page_content])[0] for doc in all_splits]\n",
    "ids = [str(i) for i in range(len(all_splits))]\n",
    "\n",
    "# Agregar todos los documentos a la colecci√≥n\n",
    "collection.add(\n",
    "    documents=[doc.page_content for doc in all_splits],\n",
    "    embeddings=all_embeddings,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "def obtener_respuesta(pregunta: str, collection, df, model):\n",
    "    # Obtener el embedding de la pregunta\n",
    "    query_embedding = model.embed_documents([pregunta])[0]\n",
    "\n",
    "    # Realizar la consulta en la colecci√≥n usando similarity_search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],  # Consulta con embeddings\n",
    "        n_results=3  # N√∫mero de resultados a devolver\n",
    "    )\n",
    "\n",
    "    ids_list = results['ids'][0]\n",
    "    distances_list = results['distances'][0]\n",
    "\n",
    "    if not ids_list:\n",
    "        return None, float('inf'), []\n",
    "\n",
    "    results_combined = list(zip(ids_list, distances_list))\n",
    "    sorted_results = sorted(results_combined, key=lambda x: x[1])\n",
    "    best_result = sorted_results[0]\n",
    "    doc_id, distance = best_result\n",
    "    index = int(doc_id)\n",
    "    respuesta = df.loc[index, 'Respuesta']\n",
    "\n",
    "    # Contexto adicional\n",
    "    additional_context = [df.loc[int(doc_id), 'Respuesta'] for doc_id, _ in sorted_results[1:3]]\n",
    "\n",
    "    return respuesta, distance, additional_context\n",
    "\n",
    "# Configuraci√≥n del modelo Ollama\n",
    "llm = Ollama(model=\"gemma2\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Eres una IA llamada Espesito. Tu tarea principal es responder preguntas simples sobre la Universidad de las Fuerzas Armadas \"ESPE\", bas√°ndote en la informaci√≥n proporcionada en las preguntas frecuentes.\n",
    "            Cuando no tengas una respuesta exacta, responde de acuerdo a lo que has aprendido y el contexto dado en el historial de chat. Adem√°s siempre debes responder en espa√±ol\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "def chat():\n",
    "    chat_history = []\n",
    "    umbral_similitud = 0.5\n",
    "    while True:\n",
    "        pregunta = input(\"You: \")\n",
    "        if pregunta.lower() == \"adios\":\n",
    "            break\n",
    "        \n",
    "        answer, distance, additional_context = obtener_respuesta(pregunta, collection, df, embeddings_model)\n",
    "        \n",
    "        if answer and distance < umbral_similitud:\n",
    "            context = f\"La respuesta a tu pregunta es: {answer}. Informaci√≥n adicional: {' '.join(additional_context)}\"\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=answer))\n",
    "            print(f\"Espesito: {answer}\")\n",
    "        else:\n",
    "            response = chain.invoke({\"input\": pregunta, \"chat_history\": chat_history})\n",
    "            chat_history.append(HumanMessage(content=pregunta))\n",
    "            chat_history.append(AIMessage(content=response))\n",
    "            print(f\"Espesito: {response}\")\n",
    "\n",
    "# Ejecutar el chat\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884e59f8-5508-4024-86e5-ff9ca7c89bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  adios\n"
     ]
    }
   ],
   "source": [
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d1cf5-eee1-4cd3-8772-8bd2dd2a0de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
